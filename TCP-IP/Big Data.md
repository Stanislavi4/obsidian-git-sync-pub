В науке о данных (Data Science) большие данные обрабатывают и анализируют, чтобы найти в них закономерности и использовать их для повышения эффективности компании или системы. Анализ больших данных в соцсетях помогает делать рекламу более релевантной для пользователей. Из-за большого объема Big Data анализируют с помощью специальных методов и больших вычислительных мощностей

## Какие данные считают «большими»
данные) — это огромные массивы информации, создаваемые IT-гигантами и большими информационными системами. Источниками больших данных являются, например, социальные сети или интернет вещей (IoT)
Big Data — это наборы данных, которые генерируются большими системами и быстро накапливаются. В 2021 году наборы больших данных измеряют в петабайтах (миллионах гигабайт). Но размер — не единственная характеристика. Такие данные обычно представлены в разных форматах, не имеют структуры, могут содержать ошибочную или нерелевантную информацию.

==Например, даже если собрать анкетные данные всех 2,5 млрд пользователей Facebook, эту базу данных нельзя назвать Big Data.== Несмотря на размер базы, информация в ней будет однородной и структурированной, а анализировать ее несложно. В то же время данные того же количества пользователей о переходах по ссылкам, лайкам и касаниям по экрану смартфона — Big Data. Таких данных очень много, они накапливаются с каждым новым сеансом в приложении и не могут быть проанализированы стандартными методами.

## Как работают с Big Data

В работе с большими данными применяют стандартный цикл Data Science из пяти этапов:

1. **Сбор.** Определение объема и структуры данных.
2. **Подготовка.** Создание архитектуры данных и очистка (Data Cleaning) от ошибок и нерелевантной информации.
3. **Обработка.** Применение математических моделей и машинного обучения. В Big Data применяют метод распределенной обработки [MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html).
4. **Анализ.** Поиск закономерностей методами Data Mining — интеллектуального анализа данных.
5. **Коммуникация (обратная связь).** Создание аналитических отчетов с предложениями о решениях на основе анализа.

Но в работе с большими данными есть особенность — необходимость большого объема памяти для хранения и вычислительных мощностей. Системы из устройств для обработки Big Data называют Hadoop-кластерами: в своей работе они используют инструменты [Apache Hadoop](https://hadoop.apache.org/). Выстраивая архитектуру «железа», руководствуются тремя принципами:

1. **Горизонтальная масштабируемость.** Big Data постоянно накапливаются и увеличиваются, поэтому система их обработки должна увеличиваться пропорционально с помощью добавления новых узлов. Если данных стало в 2 раза больше, вычислительные мощности тоже должны быть увеличены в 2 раза.
2. **Отказоустойчивость.** Вычислительных узлов в кластере может быть много и их количество увеличивается, из-за этого увеличивается вероятность выхода машин из строя. Поэтому методы работы с Big Data должны обрабатывать данные даже в случае отказа мощностей.
3. **Локальность данных.** Обычно данные распределены по большому количеству вычислительных узлов. Если физически данные находятся на одном сервере, а обрабатываются на другом, то расходы на передачу данных могут стать большими. Поэтому в Big Data стараются обрабатывать кластер данных на том же компьютере, где он хранится.

## Методы работы с Big Data

При сборе и анализе данных применяют различные методы.

**Машинное обучение и нейронные сети.** Использование в анализе нейронных сетей, которые обучаются вычислениям на массивах подготовленных данных. Обученная нейросеть может обрабатывать большие данные с большой точностью. Чтобы нейросеть работала, ее нужно сначала обучить — этот процесс называется машинным обучением. После обучения можно использовать нейросеть для обработки Big Data:

![](https://blog.skillfactory.ru/wp-content/uploads/2023/02/image5-1-3.png)

Простейшая нейросеть: здесь информация подается на входной слой, далее обрабатывается внутри, а в результате выдается через слой выхода

**Смешение и интеграция данных.** Приведение неструктурированных разнородных данных из разных источников к единому виду, например текстовому.

![](https://blog.skillfactory.ru/wp-content/uploads/2023/02/image2-3-2.png)

Схема, демонстрирующая интеграцию больших данных, которая подвергается манипуляциям/анализу. Здесь данные извлекают, очищают и обрабатывают, помещают в корпоративное хранилище данных, а после забирают для анализа

**Data Mining.** Анализ данных для выявления закономерностей в больших неструктурированных массивах данных.

![](https://blog.skillfactory.ru/wp-content/uploads/2023/02/bigdata1.png)

Data Mining — мультидисциплинарная область, возникшая и развивающаяся на базе таких наук, как прикладная статистика, распознавание образов, искусственный интеллект, теория баз данных, поэтому Data Mining объединяет разные методики и технологии работы с данными

**Краудсорсинг.** Привлечение людей к анализу данных. В небольших разовых проектах Big Data найм людей или поиск волонтеров для проверки таблиц и баз данных может оказаться дешевле, чем компьютерные методы.

![](https://blog.skillfactory.ru/wp-content/uploads/2023/02/image1-3-3.png)

Краудсорсинг-аналитика больших данных

**Предиктивная аналитика.** Анализ больших данных за прошлые периоды для прогнозирования их поведения в будущем. Например, поиск в данных клиентов параметров, которые влияли на продажи, для прогнозирования спроса на товары в будущем.

**Имитационное моделирование.** Анализ данных с возможностью изменять параметры для гипотетических ситуаций, например для вычисления объема продаж при изменении цен.